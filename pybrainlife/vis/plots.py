#!/usr/bin/env python3

import os, sys, glob
import requests
from matplotlib import colors as mcolors
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import json
import seaborn as sns
from itertools import combinations
from sklearn.metrics import mean_squared_error
from scipy import stats
import bct
import jgf
from scipy.signal import resample
import requests


### visualization related scripts
# groups data by input measure and computes mean for each value in that column. x_stat is a pd dataframe, with each row being a single value, and each column being a different ID value or measure
def average_within_column(x_stat, y_stat, x_measure, y_measure, measure):
    X = x_stat.groupby(measure).mean()[x_measure].tolist()
    Y = y_stat.groupby(measure).mean()[y_measure].tolist()

    return X, Y


# groups data by input measure and creates an array by appending data into x and y arrays. x_stat and y_stat are pd dataframes, with each row being a single value, and each column being a different ID value or measure
# designed for test retest. x_stat and y_stat should have the same number of rows. but more importantly, should correspond to the same source (i.e. subject)
# can be same pd.dataframe, but indexing of specific subject groups
def append_within_column(x_stat, y_stat, x_measure, y_measure, measure):
    X, Y = [np.array([]), np.array([])]
    for i in range(len(x_stat[measure].unique())):
        x = x_stat[x_stat[measure] == x_stat[measure].unique()[i]][x_measure]
        y = y_stat[y_stat[measure] == y_stat[measure].unique()[i]][y_measure]

        if np.isnan(x).any() or np.isnan(y).any():
            print("skipping %s due to nan" % x_stat[measure].unique()[i])
        else:
            # checks to make sure the same data
            if len(x) == len(y):
                X = np.append(X, x)
                Y = np.append(Y, y)

    return X, Y


# unravels networks. x_stat and y_stat should be S x M, where S is the number of subjects and M is the adjacency matrix for that subject
def ravel_network(x_stat, y_stat):
    import numpy as np

    X = np.ravel(x_stat).tolist()
    Y = np.ravel(y_stat).tolist()

    return X, Y


# unravels nonnetwork data. x_stat and y_stat should be pd dataframes. x_measure and y_measure are the measure to unrvavel.
# designed for test retest. x_stat and y_stat should have the same number of rows. but more importantly, should correspond to the same source (i.e. subject)
# can be same pd.dataframe, but indexing of specific subject groups
def ravel_non_network(x_stat, y_stat, x_measure, y_measure):
    X = x_stat[x_measure].to_list()
    Y = y_stat[y_measure].to_list()

    return X, Y


# wrapper function to call either of the above scripts based on user input
def setup_data(
    x_data, y_data, x_measure, y_measure, ravelAverageAppend, isnetwork, measure
):
    x_stat = x_data
    y_stat = y_data

    if ravelAverageAppend == "average":
        X, Y = average_within_column(x_stat, y_stat, x_measure, y_measure, measure)
    elif ravelAverageAppend == "append":
        X, Y = append_within_column(x_stat, y_stat, x_measure, y_measure, measure)
    elif ravelAverageAppend == "ravel":
        if isnetwork == True:
            X, Y = ravel_network(x_stat, y_stat)
        else:
            X, Y = ravel_non_network(x_stat, y_stat, x_measure, y_measure)

    return x_stat, y_stat, X, Y


# function to shuffle data and colors
def shuffle_data_alg(X, Y, hues):
    from sklearn.utils import shuffle

    X, Y, hues = shuffle(X, Y, hues)

    return X, Y, hues


# simple display or figure save function
def save_or_show_img(dir_out, x_measure, y_measure, img_name):
    import os, sys
    import matplotlib.pyplot as plt
    import warnings

    with warnings.catch_warnings():
        # this will suppress all warnings in this block
        warnings.simplefilter("ignore")

        # save or show plot
        if dir_out:
            if not os.path.exists(dir_out):
                os.mkdir(dir_out)

            if x_measure == y_measure:
                img_name_eps = img_name + "_" + x_measure + ".eps"
                img_name_png = img_name + "_" + x_measure + ".png"
                img_name_svg = img_name + "_" + x_measure + ".svg"
            else:
                img_name_eps = img_name + "_" + x_measure + "_vs_" + y_measure + ".eps"
                img_name_png = img_name + "_" + x_measure + "_vs_" + y_measure + ".png"
                img_name_svg = img_name + "_" + x_measure + "_vs_" + y_measure + ".svg"

            plt.savefig(os.path.join(dir_out, img_name_eps), transparent=True)
            plt.savefig(os.path.join(dir_out, img_name_png))
        else:
            plt.show()

        # plt.close()


# this function will identify data within 1 sd, between 1 and 2sd, and 2 sd or greater within a scatter distrbution. intended to be used when comparing a/b
# like analyses (i.e. test retest, validity, etc)
def color_distance_scatter(x, y, perfectOrSlope):
    from scipy import stats
    import numpy as np
    import math

    ### this process creates a list of sd categories by rotating the x,y distribution
    ### by 45 degrees (to make the slope essentially zero) and computing the standard
    ### deviation along the y-axis. then, it identifies whether the y-data falls either
    ### within 1 sd, within 1-2 sd, and greater than 2 sds

    # if users want to compute distribution around perfect 45 deg equality line or around
    # the actual data slope
    if perfectOrSlope == True:
        m = 1
    else:
        m, b = np.polyfit(x, y, 1)

    # compute theta as the clockwise atan rotation along m
    theta = -math.atan(m)

    # generate rotation matrix
    r = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])

    # compute rotation
    [x_dif, y_dif] = r.dot(np.array([x, y]))

    # set output variable (category)
    category = []

    # compute standard deviation thresholds for difference values
    one_sd = np.std(np.abs(y_dif))
    two_sd = one_sd * 2

    # loop through each data point and determine category (one sd: within 1 and 2 sds, two-sd: greater or equal to 2 sds, lt-one-sd: within one sd)
    category = [
        "one-sd" if one_sd <= f < two_sd else "two-sd" if f >= two_sd else "lt-one-sd"
        for f in np.abs(y_dif)
    ]

    return category


# this function will randomly subsample the data to make lighter visualization images
def subsample_data(x, y, percentage):
    data = np.array([x, y])
    subsample = np.random.choice(
        data.shape[1], int(len(data[0]) * (percentage / 100)), replace=False
    )

    sub_x = data[0, subsample]
    sub_y = data[1, subsample]

    return sub_x, sub_y


### visualization related scripts
# uses seaborn's relplot function to plot data for each unique value in a column of a pandas dataframe (ex. subjects, structureID). useful for supplementary figures or sanity checking or preliminary results
# column measure is the measure within which each unique value will have its own plot. hue_measure is the column to use for coloring the data. column_wrap is how many panels you want per row
# trendline, depending on user input, can either be the linear regression between x_data[x_measure] and y_data[y_measure] or the line of equality
# dir_out and img_name are the directory where the figures should be saved and the name for the image. will save .eps and .png
# if want to view plot instead of save, set dir_out=""
def relplot_scatter(
    x_data,
    y_data,
    x_measure,
    y_measure,
    column_measure,
    hue_measure,
    column_wrap,
    trendline,
    dir_out,
    img_name,
):
    import os, sys
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    from sklearn.metrics import mean_squared_error

    # grab data: CANNOT BE AVERAGE
    [x_stat, y_stat, X, Y] = setup_data(
        x_data, y_data, x_measure, y_measure, "ravel", False, hue_measure
    )

    p = sns.relplot(
        x=X,
        y=Y,
        col=x_stat[column_measure],
        hue=x_stat[hue_measure],
        kind="scatter",
        s=100,
        col_wrap=column_wrap,
    )

    # setting counter. looping through axes to add important info and regression lines
    i = 0
    for ax in p.axes.flat:
        x_lim, y_lim = [ax.get_xlim(), ax.get_ylim()]

        if trendline == "equality":
            ax.plot(x_lim, y_lim, ls="--", c="k")
        elif trendline == "linreg":
            m, b = np.polyfit(
                p.data[p.data[column_measure] == x_stat[column_measure].unique()[i]][
                    "x"
                ],
                p.data[p.data[column_measure] == y_stat[column_measure].unique()[i]][
                    "y"
                ],
                1,
            )
            ax.plot(ax.get_xticks(), m * ax.get_xticks() + b)
            plt.text(
                0.1,
                0.7,
                "y = %s x + %s" % (str(np.round(m, 4)), str(np.round(b, 4))),
                fontsize=12,
                verticalalignment="top",
                horizontalalignment="left",
                transform=ax.transAxes,
            )

        ax.set_xlim(x_lim)
        ax.set_ylim(y_lim)
        ax.set_xlabel(x_measure)
        ax.set_ylabel(y_measure)

        # compute correlation for each subject and add to plots
        corr = np.corrcoef(
            p.data[p.data[column_measure] == x_stat[column_measure].unique()[i]]["x"],
            p.data[p.data[column_measure] == y_stat[column_measure].unique()[i]]["y"],
        )[1][0]
        plt.text(
            0.1,
            0.9,
            "r = %s" % str(np.round(corr, 4)),
            fontsize=12,
            verticalalignment="top",
            horizontalalignment="left",
            transform=ax.transAxes,
        )

        # compute rmse for each subject and add to plots
        rmse = np.sqrt(
            mean_squared_error(
                p.data[p.data[column_measure] == x_stat[column_measure].unique()[i]][
                    "x"
                ],
                p.data[p.data[column_measure] == y_stat[column_measure].unique()[i]][
                    "y"
                ],
            )
        )
        plt.text(
            0.1,
            0.8,
            "rmse = %s" % str(np.round(rmse, 4)),
            fontsize=12,
            verticalalignment="top",
            horizontalalignment="left",
            transform=ax.transAxes,
        )

        # update counter
        i = i + 1

    # save image or show image
    save_or_show_img(dir_out, x_measure, y_measure, img_name)


# uses seaborn's scatter function to plot data from x_data[x_measure] and y_data[y_measure]. useful for publication worthy figure
# column measure is the measure within which data will be summarized. hue_measure is the column to use for coloring the data.
# ravelAverageAppend is a string value of either 'append' to use the append function, 'ravel' to use the ravel function, or 'average' to use the average function
# trendline, depending on user input, can either be the linear regression between x_data[x_measure] and y_data[y_measure] or the line of equality
# dir_out and img_name are the directory where the figures should be saved and the name for the image. will save .eps and .png
# if want to view plot instead of save, set dir_out=""
def singleplot_scatter(
    colors_dict,
    x_data,
    y_data,
    x_measure,
    y_measure,
    logX,
    column_measure,
    hue_measure,
    ravelAverageAppend,
    trendline,
    shuffleData,
    colorDistance,
    perfectOrSlope,
    subsample_percentage,
    dir_out,
    img_name,
):
    import os, sys
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    from sklearn.metrics import mean_squared_error

    # grab data
    [x_stat, y_stat, X, Y] = setup_data(
        x_data, y_data, x_measure, y_measure, ravelAverageAppend, False, column_measure
    )

    # compute corr and rmse first in case data gets subsampled later
    corr = np.corrcoef(X, Y)[1][0]
    rmse = np.sqrt(mean_squared_error(X, Y))

    # map trendlines before subsampling
    if trendline == "linreg":
        m, b = np.polyfit(X, Y, 1)
    elif trendline == "groupreg":
        for g in range(len(groups)):
            if stat_name == "volume":
                slope, intercept, r_value, p_value, std_err = stats.linregress(
                    np.log10(
                        stat[["structureID", stat_name]][
                            stat["subjectID"].str.contains("%s_" % str(g + 1))
                        ]
                        .groupby("structureID", as_index=False)
                        .mean()[stat_name]
                    ),
                    stat[["structureID", dm]][
                        stat["subjectID"].str.contains("%s_" % str(g + 1))
                    ]
                    .groupby("structureID", as_index=False)
                    .mean()[dm],
                )
            else:
                slope, intercept, r_value, p_value, std_err = stats.linregress(
                    stat[["structureID", stat_name]][
                        stat["subjectID"].str.contains("%s_" % str(g + 1))
                    ]
                    .groupby("structureID", as_index=False)
                    .mean()[stat_name],
                    stat[["structureID", dm]][
                        stat["subjectID"].str.contains("%s_" % str(g + 1))
                    ]
                    .groupby("structureID", as_index=False)
                    .mean()[dm],
                )

    if subsample_percentage:
        X, Y = subsample_data(X, Y, subsample_percentage)

    if colorDistance:
        category = color_distance_scatter(X, Y, perfectOrSlope)
    else:
        colors = sns.color_palette("colorblind", len(x_stat[hue_measure]))

    if ravelAverageAppend == "average":
        if isinstance(x_stat[hue_measure].unique()[0], str):
            hues = x_stat[hue_measure].unique().tolist()
        else:
            hues = x_stat.groupby(column_measure).mean()[hue_measure].tolist()
    else:
        hues = list(x_stat[hue_measure])

    if shuffleData == True:
        X, Y, hues = shuffle_data_alg(X, Y, hues)

    if logX == True:
        X = np.log10(X)

    if colors_dict:
        p = sns.scatterplot(
            x=X, y=Y, hue=hues, s=100, palette=colors_dict, legend=False
        )
    elif colorDistance:
        p = sns.scatterplot(
            x=X,
            y=Y,
            hue=category,
            hue_order=["two-sd", "one-sd", "lt-one-sd"],
            palette=["red", "green", "blue"],
            s=100,
        )
    else:
        p = sns.scatterplot(x=X, y=Y, hue=hues, s=100)

    # set x and ylimits, plot line of equality, and legend
    if x_measure == y_measure:
        p.axes.axis("square")
        y_ticks = p.axes.get_yticks()
        p.axes.set_xticks(y_ticks)
        p.axes.set_yticks(p.axes.get_xticks())
        p.axes.set_ylim(p.axes.get_xlim())
        p.axes.set_xlim(p.axes.get_xlim())

    x_lim, y_lim = [p.axes.get_xlim(), p.axes.get_ylim()]

    # trendline: either equality or linear regression
    if trendline == "equality":
        p.plot(x_lim, y_lim, ls="--", c="k")
        ax = plt.gca()
        ax.get_legend().remove()
    elif trendline == "linreg":
        m, b = np.polyfit(X, Y, 1)
        p.plot(p.get_xticks(), m * p.get_xticks() + b, c="k")
        plt.text(
            0.1,
            0.7,
            "y = %s x + %s" % (str(np.round(m, 4)), str(np.round(b, 4))),
            fontsize=16,
            verticalalignment="top",
            horizontalalignment="left",
            transform=p.axes.transAxes,
        )
        ax = plt.gca()
        ax.get_legend().remove()

    elif trendline == "groupreg":
        for g in range(len(groups)):
            if stat_name == "volume":
                slope, intercept, r_value, p_value, std_err = stats.linregress(
                    np.log10(
                        stat[["structureID", stat_name]][
                            stat["subjectID"].str.contains("%s_" % str(g + 1))
                        ]
                        .groupby("structureID", as_index=False)
                        .mean()[stat_name]
                    ),
                    stat[["structureID", dm]][
                        stat["subjectID"].str.contains("%s_" % str(g + 1))
                    ]
                    .groupby("structureID", as_index=False)
                    .mean()[dm],
                )
                ax = sns.regplot(
                    x=np.log10(
                        stat[["structureID", stat_name]][
                            stat["subjectID"].str.contains("%s_" % str(g + 1))
                        ]
                        .groupby("structureID", as_index=False)
                        .mean()[stat_name]
                    ),
                    y=stat[["structureID", dm]][
                        stat["subjectID"].str.contains("%s_" % str(g + 1))
                    ]
                    .groupby("structureID", as_index=False)
                    .mean()[dm],
                    color=colors[groups[g]],
                    scatter=True,
                    line_kws={"label": "y={0:.5f}x+{1:.4f}".format(slope, intercept)},
                )

            else:
                slope, intercept, r_value, p_value, std_err = stats.linregress(
                    stat[["structureID", stat_name]][
                        stat["subjectID"].str.contains("%s_" % str(g + 1))
                    ]
                    .groupby("structureID", as_index=False)
                    .mean()[stat_name],
                    stat[["structureID", dm]][
                        stat["subjectID"].str.contains("%s_" % str(g + 1))
                    ]
                    .groupby("structureID", as_index=False)
                    .mean()[dm],
                )
                ax = sns.regplot(
                    x=stat[["structureID", stat_name]][
                        stat["subjectID"].str.contains("%s_" % str(g + 1))
                    ]
                    .groupby("structureID", as_index=False)
                    .mean()[stat_name],
                    y=stat[["structureID", dm]][
                        stat["subjectID"].str.contains("%s_" % str(g + 1))
                    ]
                    .groupby("structureID", as_index=False)
                    .mean()[dm],
                    color=colors[groups[g]],
                    scatter=True,
                    line_kws={"label": "y={0:.5f}x+{1:.4f}".format(slope, intercept)},
                )

            ax.legend()

    # compute correlation for each subject and add to plots
    plt.text(
        0.1,
        0.9,
        "r = %s" % str(np.round(corr, 4)),
        fontsize=16,
        verticalalignment="top",
        horizontalalignment="left",
        transform=p.axes.transAxes,
    )

    # compute rmse for each subject and add to plots
    plt.text(
        0.1,
        0.8,
        "rmse = %s" % str(np.round(rmse, 4)),
        fontsize=16,
        verticalalignment="top",
        horizontalalignment="left",
        transform=p.axes.transAxes,
    )

    # set title and x and y labels
    plt.title("%s vs %s" % (x_measure, y_measure), fontsize=20)
    plt.xlabel(x_measure, fontsize=18)
    plt.ylabel(y_measure, fontsize=18)
    plt.xticks(fontsize=16)
    plt.yticks(fontsize=16)

    # remove top and right spines from plot
    p.axes.spines["top"].set_visible(False)
    p.axes.spines["right"].set_visible(False)

    # save image or show image
    save_or_show_img(dir_out, x_measure, y_measure, img_name)


# this function will plot group summarized tract profile data, but can also be used with any timeseries-like data
def plot_profiles(
    structures,
    stat,
    diffusion_measures,
    summary_method,
    error_method,
    dir_out,
    img_name,
):
    # loop through all structures
    for t in structures:
        print(t)
        # loop through all measures
        for dm in diffusion_measures:
            print(dm)

            imgname = img_name + "_" + t + "_" + dm

            # generate figures
            fig = plt.figure(figsize=(15, 15))

            # fig.patch.set_visible(False)
            p = plt.subplot()

            # set title and catch array for legend handle
            plt.title("%s Profiles %s: %s" % (summary_method, t, dm), fontsize=20)

            # loop through groups and plot profile data
            for g in range(len(stat.classID.unique())):
                # x is nodes
                x = stat["nodeID"].unique()

                # y is summary (mean, median, max, main) profile data
                if summary_method == "mean":
                    y = (
                        stat[stat["classID"] == stat.classID.unique()[g]]
                        .groupby(["structureID", "nodeID"])
                        .mean()[dm][t]
                    )
                elif summary_method == "median":
                    y = (
                        stat[stat["classID"] == stat.classID.unique()[g]]
                        .groupby(["structureID", "nodeID"])
                        .median()[dm][t]
                    )
                elif summary_method == "max":
                    y = (
                        stat[stat["classID"] == stat.classID.unique()[g]]
                        .groupby(["structureID", "nodeID"])
                        .max()[dm][t]
                    )
                elif summary_method == "min":
                    y = (
                        stat[stat["classID"] == stat.classID.unique()[g]]
                        .groupby(["structureID", "nodeID"])
                        .min()[dm][t]
                    )

                # error bar is either: standard error of mean (sem), standard deviation (std)
                if error_method == "sem":
                    err = stat[stat["classID"] == stat.classID.unique()[g]].groupby(
                        ["structureID", "nodeID"]
                    ).std()[dm][t] / np.sqrt(
                        len(
                            stat[stat["classID"] == stat.classID.unique()[g]][
                                "subjectID"
                            ].unique()
                        )
                    )
                else:
                    err = (
                        stat[stat["classID"] == stat.classID.unique()[g]]
                        .groupby(["structureID", "nodeID"])
                        .std()[dm][t]
                    )

                # plot summary
                plt.plot(
                    x,
                    y,
                    color=stat[stat["classID"] == stat.classID.unique()[g]][
                        "colors"
                    ].unique()[0],
                    linewidth=5,
                    label=stat.classID.unique()[g],
                )

                # plot shaded error
                plt.fill_between(
                    x,
                    y - err,
                    y + err,
                    alpha=0.2,
                    color=stat[stat["classID"] == stat.classID.unique()[g]][
                        "colors"
                    ].unique()[0],
                    label="1 %s %s" % (error_method, stat.classID.unique()[g]),
                )

            # set up labels and ticks
            plt.xlabel("Location", fontsize=18)
            plt.ylabel(dm, fontsize=18)
            plt.xticks([x[0], x[-1]], ["Begin", "End"], fontsize=16)
            plt.legend(fontsize=16)
            y_lim = plt.ylim()
            plt.yticks(
                [np.round(y_lim[0], 2), np.mean(y_lim), np.round(y_lim[1], 2)],
                fontsize=16,
            )

            # remove top and right spines from plot
            p.axes.spines["top"].set_visible(False)
            p.axes.spines["right"].set_visible(False)

            # save image or show image
            save_or_show_img(dir_out, dm, dm, imgname)
